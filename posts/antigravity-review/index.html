<!doctype html><html lang=en dir=auto data-theme=auto><link rel=stylesheet href=https://www.volatileint.dev/css/custom.css><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>A Month with Antigravity: The Good, The Bad, and The Ugly | Volatile Int</title><meta name=keywords content="ai,antigravity,dead air workflow,software engineering"><meta name=description content="A deep dive into the realities of using Antigravity, Opus 4.5, and Gemini 3 Pro for greenfield development and architectural maintenance."><meta name=author content><link rel=canonical href=https://www.volatileint.dev/posts/antigravity-review/><link crossorigin=anonymous href=https://www.volatileint.dev/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=https://www.volatileint.dev/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.volatileint.dev/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.volatileint.dev/favicon-32x32.png><link rel=apple-touch-icon href=https://www.volatileint.dev/apple-touch-icon.png><link rel=mask-icon href=https://www.volatileint.dev/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://www.volatileint.dev/posts/antigravity-review/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-N81PSXWFLZ"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-N81PSXWFLZ")}</script><meta property="og:url" content="https://www.volatileint.dev/posts/antigravity-review/"><meta property="og:site_name" content="Volatile Int"><meta property="og:title" content="A Month with Antigravity: The Good, The Bad, and The Ugly"><meta property="og:description" content="A deep dive into the realities of using Antigravity, Opus 4.5, and Gemini 3 Pro for greenfield development and architectural maintenance."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-02T12:00:00-08:00"><meta property="article:modified_time" content="2026-01-02T12:00:00-08:00"><meta property="article:tag" content="Ai"><meta property="article:tag" content="Antigravity"><meta property="article:tag" content="Dead Air Workflow"><meta property="article:tag" content="Software Engineering"><meta name=twitter:card content="summary"><meta name=twitter:title content="A Month with Antigravity: The Good, The Bad, and The Ugly"><meta name=twitter:description content="A deep dive into the realities of using Antigravity, Opus 4.5, and Gemini 3 Pro for greenfield development and architectural maintenance."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.volatileint.dev/posts/"},{"@type":"ListItem","position":2,"name":"A Month with Antigravity: The Good, The Bad, and The Ugly","item":"https://www.volatileint.dev/posts/antigravity-review/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"A Month with Antigravity: The Good, The Bad, and The Ugly","name":"A Month with Antigravity: The Good, The Bad, and The Ugly","description":"A deep dive into the realities of using Antigravity, Opus 4.5, and Gemini 3 Pro for greenfield development and architectural maintenance.","keywords":["ai","antigravity","dead air workflow","software engineering"],"articleBody":"I’ve been on paternity leave over the past few months, and between sessions playing with my son, rocking him to sleep, and changing diapers, I made a point to try and stay sharp and learn some new skills. There’s nothing more topical in the software engineering world than agentic AI for development, and I had an idea for a few new greenfield projects, so I signed up for the “pro” tier of Google’s AI infrastructure, downloaded the Antigravity IDE, and got to work on building. I entered with very little experience - I had used Claude with an outdated model a few times, but was essentially entirely new to the ecosystem. I tried out all of the models natively available in Antigravity as I could. Most of my time was spent with Opus 4.5 and Gemini 3 Pro. I gave the less powerful models a try too (such as Sonnet 4.5 and GPT-OSS 120B), but their performance was so far below the two more powerful models that I abandoned their use soon after starting.\nIn this article I’m going to share my experiences working daily with Google’s AI integrated development environment over the past month.\nThe Good Bootstrapping a Project is Fast The ability to quickly bootstrap a project from nothing is, bar none, the most exciting part of the agentic AI workflow. This is the place where most of my ideas used to die, and Antigravity has dramatically reduced that failure mode.\nI have found my projects have the most success when I focus on an end-to-end feature at the outset rather than fully fleshing out an entire layer of functionality (a practice I picked up after reading The Pragmatic Programmer). Historically, just getting to that first vertical slice was painful. Setting up a development environment, wiring together build systems, choosing libraries, and getting everything into a runnable state carried a significant upfront cost. The project needed to really be worth it to justify that effort. Many of my ideas have fizzled out before they ever got off the ground.\nWith Antigravity, I am genuinely elated at how quickly I can reach the first end-to-end features. My approach to bootstrapping is to write a very detailed description of the application. I treat the initial prompt like a lightweight design document. I describe:\nWhat we are building What software architecture I want to utilize What underlying technologies I want What build system I want I also explicitly ask the agent to create a kernel of end-to-end functionality to build off of. I describe a vertical slice of the final system that can build and run in as much detail as possible, down to the classes and interfaces I want designed. I ask it to focus on the development ergonomics of that end-to-end feature while adhering to the described architecture. These bootstrapping prompts have been multiple pages long.\nThe generated infrastructure is generally just OK. The interfaces between software modules are often unwieldy, the directory and file structure is far from what I would choose long term, and most aspects need significant massaging. Nearly all of the code in that initial infrastructure survives only through the first iteration. Even still, this approach has been an order of magnitude improvement over the traditional experience of starting a new project. There are two reasons this approach works so well:\nThe act of fixing these problems is fun. From the very first time you sit down to write any code yourself, you’re improving on a real part of your idea. It is hard to overstate the value of iterating on something tangible. Expanding and refactoring code is hard - especially so in large projects. But starting from nothing is even more difficult, and the AI agents can essentially skip that step for you. I found that the models, especially Opus 4.5, are pretty dang good at build system configuration. I use Bazel for most of my personal projects, and I can definitively say I have not written a single line of Starlark since downloading Antigravity. Tip #1: When bootstrapping a new project with an AI agent, write extremely detailed prompts. Output quality always seems to go up as my specificity increases.\nAI is a Good Chameleon I found that both Gemini 3 Pro and Opus 4.5 were very effective at implementing new features which introduced no new architectural patterns. As a concrete example, one project I am building implements an RPC (remote procedure call) software interface. It took quite a while to coerce the AI to implement the first RPCs correctly, but after that initial struggle, new RPCs following similar design patterns to existing ones have been very rapidly developed. Within a few hours working with the tools, I was able to implement new concepts utilizing existing patterns much faster than I would have been able to do it myself, and found that I have been making fewer and fewer corrections to the implementations as my prompts improved.\nOn that note, I offer an important lesson I learned. Be explicit about what pattern you wanted the AI to follow to implement a given feature - even if it feels obvious. An anecdote from that project with the RPCs will demonstrate this well: I prompted the Opus 4.5 agent to expand a service to implement the ability to update a piece of state contained within one of the application services. Similar functionality existed already elsewhere in the code via the RPC mechanism. I was very surprised when I reviewed the output to see the RPC mechanism was completely sidestepped and instead a new backdoor state update mechanism was created that opened a file to look for overrides, even when we already had dozens of other RPCs in the system implementing functionality very closely related to this new feature. I rolled back those changes, and issued the same prompt, but added another line to the prompt - “Utilize the RPC mechanism for the feature”. The resulting output was perfect and accepted without modification.\nTip #2: Be explicit about what architectural patterns to follow for a given feature request.\nAntigravity’s Plan Workflows When you prompt the AI to pursue some task, if it is of sufficient complexity, the agent will inform you that the task requires planning and approval, and generate a markdown document describing its understanding of your intent and the specific steps it will take. It will also highlight questions about the process which you are prompted to clarify.\nThis experience felt similar to talking through a new feature with a coworker. On countless occasions, the design iteration through the implementation plan unveiled some new understanding of what I wanted to happen or revealed a corner case I had not accounted for yet.\nOccasionally, the plan would ask some rather insightful questions about the architectural impacts of a change, but as will be discussed later, these insights were few and far between. The impact of a change on the larger ecosystem was rarely considered and is not one of the benefits of AI at this stage. The implementation plan was fantastic for technical implementation details of a specific, isolated code block and for system-level user-facing design impacts, not long-term maintainability.\nTip #3: Find an AI workflow that involves planning stages. But don’t rely on those plans to create something maintainable long term.\nThe Bad Antigravity UI Bugs Antigravity is a new project and no doubt has a lot of moving pieces in the background, so I’m sure these will improve with time. But there are a lot of bugs.\nAnecdotally, I’d say that you should expect the agent to crash every hour or so. These crashes manifest in one of two ways: an error that the agent could not respond and that you need to start a new conversation, or a broken file link which crashes the agent. It’s entirely opaque what causes both of these error types - perhaps dev logs exist somewhere that could provide insight, but as a user I never took the time to dig in.\nThe context of these conversations is very difficult to recover. It’s especially frustrating when the AI crashes in the middle of a feature, leaving code in a broken state which is hard to reason about. When you start a new conversation and the context is gone, the agents often struggle to recover.\nTip #4: Commit between every single buildable checkpoint in the development of a new feature. These tools are not stable, context is hard to recover, and starting over is almost always easier than recovering.\nNo Respect for the Rules While iterating on features with the agent, I found myself repeatedly specifying the same pieces of feedback. Stop creating monolithic functions, add tests around some new behavior, run the tests before saying you’re done with a feature, etc. Antigravity provides an interface to specify “rules” which the agents are supposed to take into account, defined inside markdown files. Some of the rules I attempted to utilize include:\nPrefer small functions. If you have lots of if/else logic, put the contents inside each into utility functions when possible. Generally try and keep functions less than 10 lines.\nAll components of this project are released and deployed together. You should never worry about backwards interface compatibility. If you refactor code, just delete what you don’t need instead of marking a deprecation step.\nI never had any success getting the agents to reliably follow rules like these. The output consistently violated explicit instructions, even when the rules were simple, local, and directly relevant to the code being written. Fixing these violations required constant follow-up prompts, rework, and manual cleanup. Furthermore, every corrective prompt consumes tokens while adding little to no project value. You are paying to restate constraints the tool already claims to understand. I cannot imagine using these tools in a production codebase with stricter standards, deeper abstractions, or a larger set of architectural rules. The ability to constrain agents to a set of rules they actually follow is not a nice-to-have. It is a prerequisite for using these tools in any serious engineering environment. Until that exists, the burden of enforcing standards remains entirely on the human.\nUseless Comments This complaint is specifically for Gemini 3 Pro - it writes absolutely terrible comments.\nThe comment-to-code ratio is often greater than one, and the comments rarely document anything useful. Instead of explaining intent, invariants, or non-obvious behavior, the comments restate the code, narrate obvious control flow, or serve as a kind of internal monologue. Given the earlier issue with rules being ignored, I found myself routinely following up feature prompts with a second instruction: “go back and remove all the useless comments”.\nIncluding that instruction in the initial prompt almost never works. Gemini seems to have a very strong bias toward producing copious expository comments, and the only reliable way to suppress them is with a singular, direct follow-up prompt. Amusingly, the model appears to have a decent internal understanding of what constitutes a useless comment. When explicitly asked to remove them, it does a surprisingly good job without needing further guidance.\nThe most problematic form of these comments is what I would describe as “stream-of-consciousness” commentary. These are long, rambling comment blocks where the model appears to be asking itself a question, then answering those questions, sometimes expressing shock and disbelief at what it has found. Take this hilarious example from when I asked Gemini to implement a new sort of coordinate transform in one of my projects:\n// GS position is lat/lon in msg, but we need ECI/ECEF. // Currently GroundStation message has lat/lon. LinkGeometry needs ECI. // Wait, GroundStation message HAS NO POSITION in XYZ? // We need to convert Lat/Lon to ECEF/ECI. // Assuming Earth rotation is handled elsewhere or ignored (ECEF=ECI for GS at t=0?). // Let’s implement simple LatLonToECEF.\nThis was inserted in-line with the “complete” code. When using Gemini 3 Pro, I would estimate no less than 15 percent of the generated output is expository, “stream of consciousness” comments.\nTip #5: Follow up prompts with explicit direction to remove useless comments, especially when using Gemini.\nDead-Air Workflow The primary development workflow I used followed a prompt-review-iterate loop. That time between “prompt” and “review”, especially for tasks of significant scope, can be hard to fill productively. I had hoped that I could just go off and work on some other part of the codebase, but I found that difficult for two reasons:\nThe agents produce text output as they are reasoning through a task that describes their current stage. Frequently, while watching this output, I found the AI going down a bad path. It might have been discarding some feedback I gave (there is something distinctly infuriating about reading an AI agent describe its intent to ignore your specific direction), or making a design decision which will have sweeping impacts when something much simpler is better. But whatever the reason, it’s hard to feel comfortable leaving things unsupervised. When you catch it early, you can enter a corrective comment which redirects things before they get too off the rails. So review of the thought-process output starts to become an important subtask, meaning the workflow really becomes prompt-supervise-review-iterate. If you end up touching the same files the AI is working on, you’ll create conflicts. As developers we are used to this conceptually - we use version control to resolve merge conflicts all the time - but the mechanics here are worse. Working with an AI is more like working with someone live on a Google Doc than collaborating on a git repo. Even working in completely different files is risky. If the AI builds the project and encounters a failure, for example because you are halfway through writing a function and the code is not yet buildable, the agent will see the error and attempt to fix it. In doing so, it will often delete or comment out your in-progress changes. This happened to me repeatedly. I refer to this entire phase as the dead-air portion of the workflow. You are not meaningfully productive, but you also cannot fully disengage. I have not found a better solution than either watching the agent’s reasoning output and intervening when necessary, or walking away entirely and accepting the risk that something goes seriously wrong.\nThe Ugly A Complete Disregard for Maintainability The book Software Engineering at Google defines software engineering as “programming integrated over time”. My time with Opus 4.5 and Gemini 3 Pro has solidified the suspicions I already had - these tools are abysmal at software engineering. Without oversight from a human, especially in greenfield projects with few intentionally crafted architectural patterns to immitate, the models are simply incapable of producing software which is resiliant in the face of evolving dependencies and requirements.\nOne of the projects I have been developing has a UI component. I wanted to focus my development on the backend, so I designed the interface contract between the frontend and backend, and let the AI agent fully handle developing an ncurses-based terminal UI. Long term, I planned on replacing the UI with something more polished, and so I saw this as a good opportunity to observe how the AIs handled maintenance of a component with essentially no oversight from me.\nI gave basically no direction on this UI component. I didn’t review the code that was produced. I didn’t give any feedback on how to architect the changes. I didn’t advise for or against any specific refactors. I simply communicated when I needed a new UI feature, and I would try it out after being made aware the agent was complete.\nEventually, about two weeks into the project, the UI simply became unusable. I asked to add a new feature which seemed, on its surface, completely mundane. The UI froze, and the agent spent about 45 minutes iterating, attempting to find the bug. Frustrated, I called the AI-driven-component-architecture experiment over, and took my first look at what was produced.\nThere was a single main.cpp file for the UI. The file was over 3000 lines long. It consisted of about 4 functions, with one of them being over 1500 lines long. Most of the logic was contained within a very deeply nested series of if statements inside while loops. State was shared via globals modified by lambdas which captured the globals as references. I have no idea how it ever got to this state. I would have expected these models would ship with some sort of heuristics about usability. And if not usability, that there would be some recognition from the agents that the rest of the codebase, which I had been intimately involved in architecting, was so radically different from this small corner I did not touch.\nWhile this was the most egregious misstep (well, long series of missteps) I saw an AI agent make thus far, it is far from the only one. I already mentioned the incident where an entire new state management interface was introduced for a new feature instead of using a well-established mechanism. And there are countless more examples. I cannot think of a single instance where I set out to make a new feature that introduced some novel architectural element and I did not need to intervene on some aspect of the design for the sake of sanity.\nTip #6: You have to own the architecture. At this point, AI agents are simply incapable of self-driven maintainable code.\nTip #7: Just because an AI agent is writing code in your project does not mean the architecture is unimportant. After a sufficient amount of architectural neglect, neither human nor AI will be able to make changes to the code.\nServing a Cake Half-Cooked While the architectural incompetence is the most insidious and dangerous aspect of the AI development experience, the most frustrating is when the agent claims a feature is done or an issue is addressed when it has not been. Usually this has manifested in one of two ways:\nIn the course of task execution, the AI has decided that some part of the work is “too hard”. You’ll know this happened because when you look back in the reasoning stream, you’ll find some indication that some step of the feature would drive a lot of change to some area or another, and so the functionality was stubbed out, marked TODO, or simply totally ignored. This omitted functionality is very rarely presented by the agent in its task summaries, and is only discovered via code review or actually attempting to use the features which were skipped. Less often, but frequently enough to be notable, the agent will confidently report incorrect results. I have seen agents run tests, have the tests fail, and then report they passed. I have seen agents claim a bug is resolved when it has not written a test or even attempted to build the code after it runs. Sometimes, the agent will take a much more reasonable approach to the source of the first issue. When it sees a task that becomes complicated, it will pause and prepare an explanation of the consequences of the task, and solicit feedback on its plan. I love when it does this and wish that this behavior was more consistent. I hope future iterations of the models refuse to change the scope of completion criteria without communicating the plan.\nI don’t want a half-cooked cake. I don’t want the chef to tell me that black forest cake is too hard, so it gave me an Oreo instead.\nTip #8: You have to review every single line of code. AI tools are a far cry from perfect, and they will make mistakes.\nConclusion Overall, I really have been enjoying my time with Antigravity. It took me quite a lot of time and self-reflection to extract significant value from them. In some ways, you have to pull on your interpersonal toolbelt to derive the most value. Working with the most recent models has felt a bit like working with an prodigious toddler - someone who has the ability to execute on anything you give them in the immediate future but lacks the foresight to understand the long-term consequences of their actions.\nThere’s been a lot of discourse online where software engineers make claims like “AI writes 100% of my PRs”. I can see how the agentic tools like Antigravity could consume many, maybe even most, of the programming tasks for codebases with well-established patterns and diligent engineers reviewing outputs. But, at least for now, these current models are incapable of driving maintainability. Woe unto thee who does not provide rigorous, detailed review of the outputs of an AI agent.\nWhich leads me to my last thought. I have a deep concern that these tools will produce poorly understood systems. It is simply too easy to trust the outputs of AI agents, especially when they appear to work. Especially when using the tools to write code you may not have been able to write yourself. I have been reflecting on the ncurses-based UI software component. How many new, AI-driven projects already have been installed as the backbone of critical infrastructure without sufficient architectural oversight? It seems likely to me that many such projects are running on borrowed time.\nThese tools are powerful. But it is critical to fully appreciate what they are good at, and what they are not good at. Be like Uncle Ben. Because if you rely on these tools to replace your own brain in the development process, nobody will hold the AI agent responsible.\nFeel free to reach out to me if you have any thoughts, or know how to address any of the issues I came across: sam@volatileint.dev\nIf you found this article interesting, consider subscribing to the newsletter to hear about new posts!\n","wordCount":"3634","inLanguage":"en","datePublished":"2026-01-02T12:00:00-08:00","dateModified":"2026-01-02T12:00:00-08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.volatileint.dev/posts/antigravity-review/"},"publisher":{"@type":"Organization","name":"Volatile Int","logo":{"@type":"ImageObject","url":"https://www.volatileint.dev/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://www.volatileint.dev/ accesskey=h title="  (Alt + H)"><img src=https://www.volatileint.dev/images/top-logo-light.svg alt aria-label=logo height=32></a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://www.volatileint.dev/about/ title=About><span>About</span></a></li><li><a href=https://www.volatileint.dev/newsletter/ title=Newsletter><span>Newsletter</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">A Month with Antigravity: The Good, The Bad, and The Ugly</h1><div class=post-description>A deep dive into the realities of using Antigravity, Opus 4.5, and Gemini 3 Pro for greenfield development and architectural maintenance.</div><div class=post-meta><span title='2026-01-02 12:00:00 -0800 -0800'>January 2, 2026</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#the-good aria-label="The Good">The Good</a><ul><li><a href=#bootstrapping-a-project-is-fast aria-label="Bootstrapping a Project is Fast">Bootstrapping a Project is Fast</a></li><li><a href=#ai-is-a-good-chameleon aria-label="AI is a Good Chameleon">AI is a Good Chameleon</a></li><li><a href=#antigravitys-plan-workflows aria-label="Antigravity&rsquo;s Plan Workflows">Antigravity&rsquo;s Plan Workflows</a></li></ul></li><li><a href=#the-bad aria-label="The Bad">The Bad</a><ul><li><a href=#antigravity-ui-bugs aria-label="Antigravity UI Bugs">Antigravity UI Bugs</a></li><li><a href=#no-respect-for-the-rules aria-label="No Respect for the Rules">No Respect for the Rules</a></li><li><a href=#useless-comments aria-label="Useless Comments">Useless Comments</a></li><li><a href=#dead-air-workflow aria-label="Dead-Air Workflow">Dead-Air Workflow</a></li></ul></li><li><a href=#the-ugly aria-label="The Ugly">The Ugly</a><ul><li><a href=#a-complete-disregard-for-maintainability aria-label="A Complete Disregard for Maintainability">A Complete Disregard for Maintainability</a></li><li><a href=#serving-a-cake-half-cooked aria-label="Serving a Cake Half-Cooked">Serving a Cake Half-Cooked</a></li></ul></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li></ul></div></details></div><div class=post-content><p>I&rsquo;ve been on paternity leave over the past few months, and between sessions playing with my son, rocking him to sleep, and changing diapers, I made a point to try and stay sharp and learn some new skills. There&rsquo;s nothing more topical in the software engineering world than agentic AI for development, and I had an idea for a few new greenfield projects, so I signed up for the &ldquo;pro&rdquo; tier of Google&rsquo;s AI infrastructure, downloaded the <a href=https://antigravity.google/>Antigravity</a> IDE, and got to work on building. I entered with very little experience - I had used Claude with an outdated model a few times, but was essentially entirely new to the ecosystem. I tried out all of the models natively available in Antigravity as I could. Most of my time was spent with Opus 4.5 and Gemini 3 Pro. I gave the less powerful models a try too (such as Sonnet 4.5 and GPT-OSS 120B), but their performance was so far below the two more powerful models that I abandoned their use soon after starting.</p><p>In this article I&rsquo;m going to share my experiences working daily with Google&rsquo;s AI integrated development environment over the past month.</p><h1 id=the-good>The Good<a hidden class=anchor aria-hidden=true href=#the-good>#</a></h1><h2 id=bootstrapping-a-project-is-fast>Bootstrapping a Project is <em>Fast</em><a hidden class=anchor aria-hidden=true href=#bootstrapping-a-project-is-fast>#</a></h2><p>The ability to quickly bootstrap a project from nothing is, bar none, the most exciting part of the agentic AI workflow. This is the place where most of my ideas used to die, and Antigravity has dramatically reduced that failure mode.</p><p>I have found my projects have the most success when I focus on an end-to-end feature at the outset rather than fully fleshing out an entire layer of functionality (a practice I picked up after reading <a href=https://pragprog.com/titles/tpp20/the-pragmatic-programmer-20th-anniversary-edition/>The Pragmatic Programmer</a>). Historically, just getting to that first vertical slice was painful. Setting up a development environment, wiring together build systems, choosing libraries, and getting everything into a runnable state carried a significant upfront cost. The project needed to really be worth it to justify that effort. Many of my ideas have fizzled out before they ever got off the ground.</p><p>With Antigravity, I am genuinely elated at how quickly I can reach the first end-to-end features. My approach to bootstrapping is to write a very detailed description of the application. I treat the initial prompt like a lightweight design document. I describe:</p><ul><li>What we are building</li><li>What software architecture I want to utilize</li><li>What underlying technologies I want</li><li>What build system I want</li></ul><p>I also explicitly ask the agent to create a kernel of end-to-end functionality to build off of. I describe a vertical slice of the final system that can build and run in as much detail as possible, down to the classes and interfaces I want designed. I ask it to focus on the development ergonomics of that end-to-end feature while adhering to the described architecture. These bootstrapping prompts have been multiple pages long.</p><p>The generated infrastructure is generally just OK. The interfaces between software modules are often unwieldy, the directory and file structure is far from what I would choose long term, and most aspects need significant massaging. Nearly all of the code in that initial infrastructure survives only through the first iteration. Even still, this approach has been an order of magnitude improvement over the traditional experience of starting a new project. There are two reasons this approach works so well:</p><ol><li>The act of fixing these problems is fun. From the very first time you sit down to write any code yourself, you&rsquo;re improving on a real part of your idea. It is hard to overstate the value of iterating on something tangible. Expanding and refactoring code is hard - especially so in large projects. But starting from nothing is even more difficult, and the AI agents can essentially skip that step for you.</li><li>I found that the models, especially Opus 4.5, are pretty dang good at build system configuration. I use Bazel for most of my personal projects, and I can definitively say I have not written a single line of Starlark since downloading Antigravity.</li></ol><blockquote><p>Tip #1: When bootstrapping a new project with an AI agent, write extremely detailed prompts. Output quality always seems to go up as my specificity increases.</p></blockquote><h2 id=ai-is-a-good-chameleon>AI is a Good Chameleon<a hidden class=anchor aria-hidden=true href=#ai-is-a-good-chameleon>#</a></h2><p>I found that both Gemini 3 Pro and Opus 4.5 were very effective at implementing new features which introduced no new architectural patterns. As a concrete example, one project I am building implements an RPC (remote procedure call) software interface. It took quite a while to coerce the AI to implement the first RPCs correctly, but after that initial struggle, new RPCs following similar design patterns to existing ones have been very rapidly developed. Within a few hours working with the tools, I was able to implement new concepts utilizing existing patterns much faster than I would have been able to do it myself, and found that I have been making fewer and fewer corrections to the implementations as my prompts improved.</p><p>On that note, I offer an important lesson I learned. Be explicit about what pattern you wanted the AI to follow to implement a given feature - even if it feels obvious. An anecdote from that project with the RPCs will demonstrate this well: I prompted the Opus 4.5 agent to expand a service to implement the ability to update a piece of state contained within one of the application services. Similar functionality existed already elsewhere in the code via the RPC mechanism. I was very surprised when I reviewed the output to see the RPC mechanism was completely sidestepped and instead a new backdoor state update mechanism was created that opened a file to look for overrides, even when we already had dozens of other RPCs in the system implementing functionality very closely related to this new feature. I rolled back those changes, and issued the same prompt, but added another line to the prompt - &ldquo;Utilize the RPC mechanism for the feature&rdquo;. The resulting output was perfect and accepted without modification.</p><blockquote><p>Tip #2: Be explicit about what architectural patterns to follow for a given feature request.</p></blockquote><h2 id=antigravitys-plan-workflows>Antigravity&rsquo;s Plan Workflows<a hidden class=anchor aria-hidden=true href=#antigravitys-plan-workflows>#</a></h2><p>When you prompt the AI to pursue some task, if it is of sufficient complexity, the agent will inform you that the task requires planning and approval, and generate a markdown document describing its understanding of your intent and the specific steps it will take. It will also highlight questions about the process which you are prompted to clarify.</p><p>This experience felt similar to talking through a new feature with a coworker. On countless occasions, the design iteration through the implementation plan unveiled some new understanding of what I wanted to happen or revealed a corner case I had not accounted for yet.</p><p>Occasionally, the plan would ask some rather insightful questions about the architectural impacts of a change, but as will be discussed later, these insights were few and far between. The impact of a change on the larger ecosystem was rarely considered and is not one of the benefits of AI at this stage. The implementation plan was fantastic for technical implementation details of a specific, isolated code block and for system-level user-facing design impacts, not long-term maintainability.</p><blockquote><p>Tip #3: Find an AI workflow that involves planning stages. But don&rsquo;t rely on those plans to create something maintainable long term.</p></blockquote><h1 id=the-bad>The Bad<a hidden class=anchor aria-hidden=true href=#the-bad>#</a></h1><h2 id=antigravity-ui-bugs>Antigravity UI Bugs<a hidden class=anchor aria-hidden=true href=#antigravity-ui-bugs>#</a></h2><p>Antigravity is a new project and no doubt has a lot of moving pieces in the background, so I&rsquo;m sure these will improve with time. But there are a lot of bugs.</p><p>Anecdotally, I&rsquo;d say that you should expect the agent to crash every hour or so. These crashes manifest in one of two ways: an error that the agent could not respond and that you need to start a new conversation, or a broken file link which crashes the agent. It&rsquo;s entirely opaque what causes both of these error types - perhaps dev logs exist somewhere that could provide insight, but as a user I never took the time to dig in.</p><p>The context of these conversations is very difficult to recover. It&rsquo;s especially frustrating when the AI crashes in the middle of a feature, leaving code in a broken state which is hard to reason about. When you start a new conversation and the context is gone, the agents often struggle to recover.</p><blockquote><p>Tip #4: Commit between every single buildable checkpoint in the development of a new feature. These tools are not stable, context is hard to recover, and starting over is almost always easier than recovering.</p></blockquote><h2 id=no-respect-for-the-rules>No Respect for the Rules<a hidden class=anchor aria-hidden=true href=#no-respect-for-the-rules>#</a></h2><p>While iterating on features with the agent, I found myself repeatedly specifying the same pieces of feedback. Stop creating monolithic functions, add tests around some new behavior, run the tests before saying you&rsquo;re done with a feature, etc. Antigravity provides an interface to specify &ldquo;rules&rdquo; which the agents are supposed to take into account, defined inside markdown files. Some of the rules I attempted to utilize include:</p><blockquote><p>Prefer small functions. If you have lots of if/else logic, put the contents inside each into utility functions when possible. Generally try and keep functions less than 10 lines.</p></blockquote><blockquote><p>All components of this project are released and deployed together. You should never worry about backwards interface compatibility. If you refactor code, just delete what you don&rsquo;t need instead of marking a deprecation step.</p></blockquote><p>I never had any success getting the agents to reliably follow rules like these. The output consistently violated explicit instructions, even when the rules were simple, local, and directly relevant to the code being written. Fixing these violations required constant follow-up prompts, rework, and manual cleanup. Furthermore, every corrective prompt consumes tokens while adding little to no project value. You are paying to restate constraints the tool already claims to understand. I cannot imagine using these tools in a production codebase with stricter standards, deeper abstractions, or a larger set of architectural rules. The ability to constrain agents to a set of rules they actually follow is not a nice-to-have. It is a prerequisite for using these tools in any serious engineering environment. Until that exists, the burden of enforcing standards remains entirely on the human.</p><h2 id=useless-comments>Useless Comments<a hidden class=anchor aria-hidden=true href=#useless-comments>#</a></h2><p>This complaint is specifically for Gemini 3 Pro - it writes absolutely terrible comments.</p><p>The comment-to-code ratio is often greater than one, and the comments rarely document anything useful. Instead of explaining intent, invariants, or non-obvious behavior, the comments restate the code, narrate obvious control flow, or serve as a kind of internal monologue. Given the earlier issue with rules being ignored, I found myself routinely following up feature prompts with a second instruction: “go back and remove all the useless comments”.</p><p>Including that instruction in the initial prompt almost never works. Gemini seems to have a very strong bias toward producing copious expository comments, and the only reliable way to suppress them is with a singular, direct follow-up prompt. Amusingly, the model appears to have a decent internal understanding of what constitutes a useless comment. When explicitly asked to remove them, it does a surprisingly good job without needing further guidance.</p><p>The most problematic form of these comments is what I would describe as &ldquo;stream-of-consciousness&rdquo; commentary. These are long, rambling comment blocks where the model appears to be asking itself a question, then answering those questions, sometimes expressing shock and disbelief at what it has found. Take this hilarious example from when I asked Gemini to implement a new sort of coordinate transform in one of my projects:</p><blockquote><p>// GS position is lat/lon in msg, but we need ECI/ECEF.
// Currently GroundStation message has lat/lon. LinkGeometry needs ECI.
// Wait, GroundStation message HAS NO POSITION in XYZ?
// We need to convert Lat/Lon to ECEF/ECI.
// Assuming Earth rotation is handled elsewhere or ignored (ECEF=ECI for GS at t=0?).
// Let&rsquo;s implement simple LatLonToECEF.</p></blockquote><p>This was inserted in-line with the &ldquo;complete&rdquo; code. When using Gemini 3 Pro, I would estimate no less than 15 percent of the generated output is expository, &ldquo;stream of consciousness&rdquo; comments.</p><blockquote><p>Tip #5: Follow up prompts with explicit direction to remove useless comments, especially when using Gemini.</p></blockquote><h2 id=dead-air-workflow>Dead-Air Workflow<a hidden class=anchor aria-hidden=true href=#dead-air-workflow>#</a></h2><p>The primary development workflow I used followed a prompt-review-iterate loop. That time between &ldquo;prompt&rdquo; and &ldquo;review&rdquo;, especially for tasks of significant scope, can be hard to fill productively. I had hoped that I could just go off and work on some other part of the codebase, but I found that difficult for two reasons:</p><ol><li>The agents produce text output as they are reasoning through a task that describes their current stage. Frequently, while watching this output, I found the AI going down a bad path. It might have been discarding some feedback I gave (there is something distinctly infuriating about reading an AI agent describe its intent to ignore your specific direction), or making a design decision which will have sweeping impacts when something much simpler is better. But whatever the reason, it&rsquo;s hard to feel comfortable leaving things unsupervised. When you catch it early, you can enter a corrective comment which redirects things before they get too off the rails. So review of the thought-process output starts to become an important subtask, meaning the workflow really becomes prompt-supervise-review-iterate.</li><li>If you end up touching the same files the AI is working on, you&rsquo;ll create conflicts. As developers we are used to this conceptually - we use version control to resolve merge conflicts all the time - but the mechanics here are worse. Working with an AI is more like working with someone live on a Google Doc than collaborating on a git repo.</li><li>Even working in completely different files is risky. If the AI builds the project and encounters a failure, for example because you are halfway through writing a function and the code is not yet buildable, the agent will see the error and attempt to fix it. In doing so, it will often delete or comment out your in-progress changes. This happened to me repeatedly.</li></ol><p>I refer to this entire phase as the dead-air portion of the workflow. You are not meaningfully productive, but you also cannot fully disengage. I have not found a better solution than either watching the agent’s reasoning output and intervening when necessary, or walking away entirely and accepting the risk that something goes seriously wrong.</p><h1 id=the-ugly>The Ugly<a hidden class=anchor aria-hidden=true href=#the-ugly>#</a></h1><h2 id=a-complete-disregard-for-maintainability>A Complete Disregard for Maintainability<a hidden class=anchor aria-hidden=true href=#a-complete-disregard-for-maintainability>#</a></h2><p>The book <a href=https://abseil.io/resources/swe-book>Software Engineering at Google</a> defines software engineering as &ldquo;programming integrated over time&rdquo;. My time with Opus 4.5 and Gemini 3 Pro has solidified the suspicions I already had - these tools are abysmal at software engineering. Without oversight from a human, especially in greenfield projects with few intentionally crafted architectural patterns to immitate, the models are simply incapable of producing software which is resiliant in the face of evolving dependencies and requirements.</p><p>One of the projects I have been developing has a UI component. I wanted to focus my development on the backend, so I designed the interface contract between the frontend and backend, and let the AI agent fully handle developing an ncurses-based terminal UI. Long term, I planned on replacing the UI with something more polished, and so I saw this as a good opportunity to observe how the AIs handled maintenance of a component with essentially no oversight from me.</p><p>I gave basically no direction on this UI component. I didn&rsquo;t review the code that was produced. I didn&rsquo;t give any feedback on how to architect the changes. I didn&rsquo;t advise for or against any specific refactors. I simply communicated when I needed a new UI feature, and I would try it out after being made aware the agent was complete.</p><p>Eventually, about two weeks into the project, the UI simply became unusable. I asked to add a new feature which seemed, on its surface, completely mundane. The UI froze, and the agent spent about 45 minutes iterating, attempting to find the bug. Frustrated, I called the AI-driven-component-architecture experiment over, and took my first look at what was produced.</p><ol><li>There was a single <code>main.cpp</code> file for the UI.</li><li>The file was over 3000 lines long.</li><li>It consisted of about 4 functions, with one of them being over 1500 lines long.</li><li>Most of the logic was contained within a <em>very</em> deeply nested series of <code>if</code> statements inside <code>while</code> loops.</li><li>State was shared via globals modified by lambdas which captured the globals as references.</li></ol><p>I have no idea how it ever got to this state. I would have expected these models would ship with some sort of heuristics about usability. And if not usability, that there would be some recognition from the agents that the rest of the codebase, which I had been intimately involved in architecting, was so radically different from this small corner I did not touch.</p><p>While this was the most egregious misstep (well, long series of missteps) I saw an AI agent make thus far, it is far from the only one. I already mentioned the incident where an entire new state management interface was introduced for a new feature instead of using a well-established mechanism. And there are countless more examples. I cannot think of a single instance where I set out to make a new feature that introduced some novel architectural element and I did not need to intervene on some aspect of the design for the sake of sanity.</p><blockquote><p>Tip #6: You have to own the architecture. At this point, AI agents are simply incapable of self-driven maintainable code.</p></blockquote><blockquote><p>Tip #7: Just because an AI agent is writing code in your project does not mean the architecture is unimportant. After a sufficient amount of architectural neglect, neither human nor AI will be able to make changes to the code.</p></blockquote><h2 id=serving-a-cake-half-cooked>Serving a Cake Half-Cooked<a hidden class=anchor aria-hidden=true href=#serving-a-cake-half-cooked>#</a></h2><p>While the architectural incompetence is the most insidious and dangerous aspect of the AI development experience, the most frustrating is when the agent claims a feature is done or an issue is addressed when it has not been. Usually this has manifested in one of two ways:</p><ol><li>In the course of task execution, the AI has decided that some part of the work is &ldquo;too hard&rdquo;. You&rsquo;ll know this happened because when you look back in the reasoning stream, you&rsquo;ll find some indication that some step of the feature would drive a lot of change to some area or another, and so the functionality was stubbed out, marked TODO, or simply totally ignored. This omitted functionality is very rarely presented by the agent in its task summaries, and is only discovered via code review or actually attempting to use the features which were skipped.</li><li>Less often, but frequently enough to be notable, the agent will confidently report incorrect results. I have seen agents run tests, have the tests fail, and then report they passed. I have seen agents claim a bug is resolved when it has not written a test or even attempted to build the code after it runs.</li></ol><p>Sometimes, the agent will take a much more reasonable approach to the source of the first issue. When it sees a task that becomes complicated, it will pause and prepare an explanation of the consequences of the task, and solicit feedback on its plan. I love when it does this and wish that this behavior was more consistent. I hope future iterations of the models refuse to change the scope of completion criteria without communicating the plan.</p><p>I don&rsquo;t want a half-cooked cake. I don&rsquo;t want the chef to tell me that black forest cake is too hard, so it gave me an Oreo instead.</p><blockquote><p>Tip #8: You have to review every single line of code. AI tools are a far cry from perfect, and they <em>will</em> make mistakes.</p></blockquote><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1><p>Overall, I really have been enjoying my time with Antigravity. It took me quite a lot of time and self-reflection to extract significant value from them. In some ways, you have to pull on your interpersonal toolbelt to derive the most value. Working with the most recent models has felt a bit like working with an prodigious toddler - someone who has the ability to execute on anything you give them in the immediate future but lacks the foresight to understand the long-term consequences of their actions.</p><p>There&rsquo;s been a lot of discourse online where software engineers make claims like &ldquo;AI writes 100% of my PRs&rdquo;. I can see how the agentic tools like Antigravity could consume many, maybe even most, of the programming tasks for codebases with well-established patterns and diligent engineers reviewing outputs. But, at least for now, these current models are incapable of driving maintainability. Woe unto thee who does not provide rigorous, detailed review of the outputs of an AI agent.</p><p>Which leads me to my last thought. I have a deep concern that these tools will produce poorly understood systems. It is simply too easy to trust the outputs of AI agents, especially when they appear to work. Especially when using the tools to write code you may not have been able to write yourself. I have been reflecting on the ncurses-based UI software component. How many new, AI-driven projects already have been installed as the backbone of critical infrastructure without sufficient architectural oversight? It seems likely to me that many such projects are running on borrowed time.</p><p>These tools are powerful. But it is critical to fully appreciate what they are good at, and what they are not good at. Be like <a href="https://www.youtube.com/watch?v=guuYU74wU70">Uncle Ben</a>. Because if you rely on these tools to replace your own brain in the development process, nobody will hold the AI agent responsible.</p><p>Feel free to reach out to me if you have any thoughts, or know how to address any of the issues I came across: <a href=mailto:sam@volatileint.dev>sam@volatileint.dev</a></p><p>If you found this article interesting, consider subscribing to the <a href=https://volatileint.dev/newsletter>newsletter</a> to hear about new posts!</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://www.volatileint.dev/tags/ai/>Ai</a></li><li><a href=https://www.volatileint.dev/tags/antigravity/>Antigravity</a></li><li><a href=https://www.volatileint.dev/tags/dead-air-workflow/>Dead Air Workflow</a></li><li><a href=https://www.volatileint.dev/tags/software-engineering/>Software Engineering</a></li></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share A Month with Antigravity: The Good, The Bad, and The Ugly on x" href="https://x.com/intent/tweet/?text=A%20Month%20with%20Antigravity%3a%20The%20Good%2c%20The%20Bad%2c%20and%20The%20Ugly&amp;url=https%3a%2f%2fwww.volatileint.dev%2fposts%2fantigravity-review%2f&amp;hashtags=ai%2cantigravity%2cdeadairworkflow%2csoftwareengineering"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Month with Antigravity: The Good, The Bad, and The Ugly on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fwww.volatileint.dev%2fposts%2fantigravity-review%2f&amp;title=A%20Month%20with%20Antigravity%3a%20The%20Good%2c%20The%20Bad%2c%20and%20The%20Ugly&amp;summary=A%20Month%20with%20Antigravity%3a%20The%20Good%2c%20The%20Bad%2c%20and%20The%20Ugly&amp;source=https%3a%2f%2fwww.volatileint.dev%2fposts%2fantigravity-review%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Month with Antigravity: The Good, The Bad, and The Ugly on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fwww.volatileint.dev%2fposts%2fantigravity-review%2f&title=A%20Month%20with%20Antigravity%3a%20The%20Good%2c%20The%20Bad%2c%20and%20The%20Ugly"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Month with Antigravity: The Good, The Bad, and The Ugly on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwww.volatileint.dev%2fposts%2fantigravity-review%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Month with Antigravity: The Good, The Bad, and The Ugly on whatsapp" href="https://api.whatsapp.com/send?text=A%20Month%20with%20Antigravity%3a%20The%20Good%2c%20The%20Bad%2c%20and%20The%20Ugly%20-%20https%3a%2f%2fwww.volatileint.dev%2fposts%2fantigravity-review%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Month with Antigravity: The Good, The Bad, and The Ugly on telegram" href="https://telegram.me/share/url?text=A%20Month%20with%20Antigravity%3a%20The%20Good%2c%20The%20Bad%2c%20and%20The%20Ugly&amp;url=https%3a%2f%2fwww.volatileint.dev%2fposts%2fantigravity-review%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Month with Antigravity: The Good, The Bad, and The Ugly on ycombinator" href="https://news.ycombinator.com/submitlink?t=A%20Month%20with%20Antigravity%3a%20The%20Good%2c%20The%20Bad%2c%20and%20The%20Ugly&u=https%3a%2f%2fwww.volatileint.dev%2fposts%2fantigravity-review%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://www.volatileint.dev/>Volatile Int</a></span> ·
<span><a href=https://www.volatileint.dev/index.xml>Subscribe via RSS</a>
</span>·
<span><a href=https://buttondown.com/volatileint>Subscribe via Newsletter</a>
</span>·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:!0})</script></body></html>